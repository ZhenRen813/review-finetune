# train_adapter.py
import os
import ast
import torch
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    DataCollatorForLanguageModeling,
    Trainer,
    TrainingArguments
)
from peft import get_peft_model, LoraConfig

# 1. æ¨¡å‹ä»“åº“ IDï¼ˆè¿œç¨‹åŠ è½½ï¼Œæ— éœ€æœ¬åœ° cloneï¼‰
model_id = "unsloth/DeepSeek-R1-0528-Qwen3-8B"

# 2. åŠ è½½ tokenizer å’Œ æ¨¡å‹ï¼ˆè‡ªåŠ¨ä¸‹è½½ sharded safetensorsï¼‰
tokenizer = AutoTokenizer.from_pretrained(
    model_id,
    trust_remote_code=True
)
# ä½¿ç”¨é»˜è®¤ dtypeï¼ˆFP32ï¼‰ä»¥å…¼å®¹ Mac M-series æ—  GPU æ”¯æŒæƒ…å†µ
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="cpu",
    trust_remote_code=True
)

# 3. æ³¨å…¥ LoRA Adapter
peft_config = LoraConfig(
    task_type="CAUSAL_LM",
    inference_mode=False,
    r=8,
    lora_alpha=16,
    lora_dropout=0.05,
    target_modules=["q_proj", "v_proj"],
    bias="none"
)
model = get_peft_model(model, peft_config)

# å¼ºåˆ¶å°†æ¨¡å‹ç½®äº CPUï¼ˆmacOS æ—  GPU æ”¯æŒï¼‰
model.to("cpu")

# 4. è‡ªå®šä¹‰ ABSA æ•°æ®é›†
class ABSADataset(torch.utils.data.Dataset):
    def __init__(self, filepath, tokenizer, max_length=512):
        self.examples = []
        with open(filepath, encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if not line or '####' not in line:
                    continue
                text, label_str = line.split('####', 1)
                labels = ast.literal_eval(label_str)
                prompt = f"è¯„è®º: {text}####"
                response = str(labels)
                full = prompt + response
                tokenized = tokenizer(
                    full,
                    truncation=True,
                    max_length=max_length,
                    padding='max_length'
                )
                tokenized['labels'] = tokenized['input_ids'].copy()
                self.examples.append(tokenized)

    def __len__(self):
        return len(self.examples)

    def __getitem__(self, idx):
        return {k: torch.tensor(v) for k, v in self.examples[idx].items()}

# 5. åŠ è½½æ•°æ®é›†æ–‡ä»¶
train_dataset = ABSADataset('train.txt', tokenizer)
dev_dataset   = ABSADataset('dev.txt', tokenizer)

data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)

# 6. è®­ç»ƒå‚æ•°é…ç½®ï¼ˆç§»é™¤ bf16 ä»¥å…¼å®¹æ—  GPU ç¯å¢ƒï¼‰
training_args = TrainingArguments(
    output_dir='./lora_adapter_output',
    per_device_train_batch_size=1,
    per_device_eval_batch_size=1,
    gradient_accumulation_steps=4,
    eval_strategy='steps',
    eval_steps=300,
    logging_steps=50,
    num_train_epochs=3,
    learning_rate=2e-4,
    save_total_limit=2,
    save_steps=2,
    push_to_hub=False
)

# 7. æ£€æŸ¥æ˜¯å¦æ¢å¤è®­ç»ƒ
last_checkpoint = None
if os.path.isdir('./lora_adapter_output/checkpoint-936'):
    last_checkpoint = './lora_adapter_output/checkpoint-936'
    print(f"æ£€æµ‹åˆ°æ£€æŸ¥ç‚¹ï¼Œå‡†å¤‡ä» {last_checkpoint} æ¢å¤è®­ç»ƒ...")

# 8. åˆå§‹åŒ– Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=dev_dataset,
    data_collator=data_collator
)

# 9. å¼€å§‹è®­ç»ƒï¼ˆæ”¯æŒæ–­ç‚¹æ¢å¤ï¼‰
trainer.train(resume_from_checkpoint=last_checkpoint)

# 10. ä¿å­˜æœ€ç»ˆæ¨¡å‹å’Œtokenizer
os.makedirs('./lora_adapter_output', exist_ok=True)
model.save_pretrained('./lora_adapter_output')
tokenizer.save_pretrained('./lora_adapter_output')
print("âœ… LoRA adapter å·²ä¿å­˜åˆ° ./lora_adapter_output")

# # 7. åˆå§‹åŒ– Trainer å¹¶å¼€å§‹è®­ç»ƒ
# trainer = Trainer(
#     model=model,
#     args=training_args,
#     train_dataset=train_dataset,
#     eval_dataset=dev_dataset,
#     data_collator=data_collator
# )

# trainer.train()

# # 8. ä¿å­˜ LoRA Adapter å’Œ tokenizer
# os.makedirs('./lora_adapter_output', exist_ok=True)
# model.save_pretrained('./lora_adapter_output')
# tokenizer.save_pretrained('./lora_adapter_output')
# print("LoRA adapter å·²ä¿å­˜åˆ° ./lora_adapter_output")

# 9. æ¨ç†ç¤ºä¾‹
if __name__ == '__main__':
    from peft import PeftModel
    base = AutoModelForCausalLM.from_pretrained(
        model_id,
        device_map="cpu",
        trust_remote_code=True
    )
    adapter = PeftModel.from_pretrained(base, './lora_adapter_output')
    adapter.eval()
    text = "è¿™å®¶é…’åº—çš„æˆ¿é—´å¾ˆèˆ’æœ,ä½†æ˜¯æœåŠ¡äººå‘˜æ€åº¦å¾ˆå·®."
    prompt = f"è¯„è®º: {text}####"
    inputs = tokenizer(prompt, return_tensors="pt").to(adapter.device)
    out = adapter.generate(**inputs, max_new_tokens=128)
    result = tokenizer.decode(out[0], skip_special_tokens=True)
    print("æ¨ç†ç»“æœ:", result)


# # train_adapter.py
# import os
# import ast
# import torch
# from transformers import (
#     AutoModelForCausalLM,
#     AutoTokenizer,
#     DataCollatorForLanguageModeling,
#     Trainer,
#     TrainingArguments
# )
# from peft import get_peft_model, LoraConfig

# # 1. æ¨¡å‹ä»“åº“ ID
# model_id = "unsloth/DeepSeek-R1-0528-Qwen3-8B"

# # 2. åŠ è½½ tokenizer å’Œæ¨¡å‹
# tokenizer = AutoTokenizer.from_pretrained(
#     model_id,
#     trust_remote_code=True
# )
# model = AutoModelForCausalLM.from_pretrained(
#     model_id,
#     trust_remote_code=True,
#     device_map="cpu"  # Mac æ—  GPU ä½¿ç”¨ CPU
# )

# # 3. æ³¨å…¥ LoRA Adapter
# peft_config = LoraConfig(
#     task_type="CAUSAL_LM",
#     inference_mode=False,
#     r=8,
#     lora_alpha=16,
#     lora_dropout=0.05,
#     target_modules=["q_proj", "v_proj"],
#     bias="none"
# )
# model = get_peft_model(model, peft_config)
# model.to("cpu")  # æ˜ç¡®ç½®äº CPU

# # 4. è‡ªå®šä¹‰ ABSA æ•°æ®é›†
# class ABSADataset(torch.utils.data.Dataset):
#     def __init__(self, filepath, tokenizer, max_length=512):
#         self.examples = []
#         with open(filepath, encoding='utf-8') as f:
#             for line in f:
#                 line = line.strip()
#                 if not line or '####' not in line:
#                     continue
#                 text, label_str = line.split('####', 1)
#                 labels = ast.literal_eval(label_str)
#                 prompt = f"è¯„è®º: {text}####"
#                 response = str(labels)
#                 full = prompt + response
#                 tokenized = tokenizer(
#                     full,
#                     truncation=True,
#                     max_length=max_length,
#                     padding='max_length'
#                 )
#                 tokenized['labels'] = tokenized['input_ids'].copy()
#                 self.examples.append(tokenized)

#     def __len__(self):
#         return len(self.examples)

#     def __getitem__(self, idx):
#         return {k: torch.tensor(v) for k, v in self.examples[idx].items()}

# # 5. åŠ è½½æ•°æ®
# train_dataset = ABSADataset('train.txt', tokenizer)
# dev_dataset   = ABSADataset('dev.txt', tokenizer)
# data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)

# # 6. è®­ç»ƒå‚æ•°
# training_args = TrainingArguments(
#     output_dir='./lora_adapter_output',
#     per_device_train_batch_size=1,
#     per_device_eval_batch_size=1,
#     gradient_accumulation_steps=4,
#     evaluation_strategy='steps',
#     eval_steps=100,
#     logging_steps=50,
#     num_train_epochs=3,
#     learning_rate=2e-4,
#     save_total_limit=2,
#     save_steps=8,  # æ¯8æ­¥ä¿å­˜
#     push_to_hub=False
# )

# # 7. æ£€æŸ¥æ˜¯å¦æ¢å¤è®­ç»ƒ
# last_checkpoint = None
# if os.path.isdir('./lora_adapter_output/checkpoint-last'):
#     last_checkpoint = './lora_adapter_output/checkpoint-last'
#     print(f"æ£€æµ‹åˆ°æ£€æŸ¥ç‚¹ï¼Œå‡†å¤‡ä» {last_checkpoint} æ¢å¤è®­ç»ƒ...")

# # 8. åˆå§‹åŒ– Trainer
# trainer = Trainer(
#     model=model,
#     args=training_args,
#     train_dataset=train_dataset,
#     eval_dataset=dev_dataset,
#     data_collator=data_collator
# )

# # 9. å¼€å§‹è®­ç»ƒï¼ˆæ”¯æŒæ–­ç‚¹æ¢å¤ï¼‰
# trainer.train(resume_from_checkpoint=last_checkpoint)

# # 10. ä¿å­˜æœ€ç»ˆæ¨¡å‹å’Œtokenizer
# os.makedirs('./lora_adapter_output', exist_ok=True)
# model.save_pretrained('./lora_adapter_output')
# tokenizer.save_pretrained('./lora_adapter_output')
# print("âœ… LoRA adapter å·²ä¿å­˜åˆ° ./lora_adapter_output")

# # 11. æ¨ç†æµ‹è¯•ï¼ˆå¯é€‰ï¼‰
# if __name__ == '__main__':
#     from peft import PeftModel
#     base = AutoModelForCausalLM.from_pretrained(
#         model_id,
#         trust_remote_code=True,
#         device_map="cpu"
#     )
#     adapter = PeftModel.from_pretrained(base, './lora_adapter_output')
#     adapter.eval()
#     text = "è¿™å®¶é…’åº—çš„æˆ¿é—´å¾ˆèˆ’æœ,ä½†æ˜¯æœåŠ¡äººå‘˜æ€åº¦å¾ˆå·®."
#     prompt = f"è¯„è®º: {text}####"
#     inputs = tokenizer(prompt, return_tensors="pt").to(adapter.device)
#     out = adapter.generate(**inputs, max_new_tokens=128)
#     result = tokenizer.decode(out[0], skip_special_tokens=True)
#     print("ğŸ” æ¨ç†ç»“æœ:", result)
